import boto3
import gzip
import json
import io
import csv
from datetime import datetime

s3 = boto3.client('s3')

def parse_json_line(line):
    """Parse potentially concatenated JSON objects from a line."""
    line = line.strip()
    if not line:
        return None

    all_records = []
    pos = 0
    while pos < len(line):
        try:
            # Find opening brace
            while pos < len(line) and line[pos] != '{':
                pos += 1
            if pos >= len(line):
                break

            # Parse individual JSON object
            decoder = json.JSONDecoder()
            record, new_pos = decoder.raw_decode(line[pos:])
            all_records.append(record)
            pos += new_pos
        except json.JSONDecodeError:
            pos += 1
        except Exception as e:
            print(f"‚ùå Error parsing at position {pos}: {e}")
            pos += 1

    return all_records[0] if all_records else None

def infer_data_type(record):
    """Infer the data type based on record fields."""
    if 'PackageID' in record:
        return 'package'
    elif 'RobotName' in record and 'TaskType' in record:
        return 'robot_telemetry'
    elif 'RobotName' in record and 'FailureReason' in record:
        return 'robot_failure'
    return 'unknown'

def lambda_handler(event, context):
    try:
        source_bucket = event['detail']['bucket']['name']
        source_key = event['detail']['object']['key']
        print(f"üîÑ Processing GZIP file: {source_key}")

        # Read and decompress GZIP
        response = s3.get_object(Bucket=source_bucket, Key=source_key)
        with gzip.GzipFile(fileobj=io.BytesIO(response['Body'].read())) as gz:
            content = gz.read().decode('utf-8')

        # Add debug logging
        print(f"üìù Raw content length: {len(content)}")
        print(f"üìù First few lines: {content[:500]}")

        # Process content as one line since Firehose may concatenate records
        records = []
        line = content.strip()
        pos = 0
        while pos < len(line):
            try:
                # Find opening brace
                while pos < len(line) and line[pos] != '{':
                    pos += 1
                if pos >= len(line):
                    break

                # Parse individual JSON object
                decoder = json.JSONDecoder()
                record, new_pos = decoder.raw_decode(line[pos:])
                records.append(record)
                print(f"‚úÖ Parsed record {len(records)}: {record.get('PackageID') or record.get('RobotName')}")
                pos += new_pos
            except json.JSONDecodeError:
                pos += 1
            except Exception as e:
                print(f"‚ùå Error parsing at position {pos}: {e}")
                pos += 1

        print(f"üì¶ Successfully parsed {len(records)} records")

        # Group by data type
        data_by_type = {
            'package': [],
            'robot_telemetry': [],
            'robot_failure': []
        }

        for record in records:
            # Infer data_type if not present
            data_type = record.get('data_type') or infer_data_type(record)
            if data_type in data_by_type:
                data_by_type[data_type].append(record)
                print(f"üìé Added record to {data_type}: {record.get('PackageID') or record.get('RobotName')}")

        # Print debug info for each data type
        for dtype, recs in data_by_type.items():
            print(f"üìä {dtype}: {len(recs)} records")

        # Write CSV files
        destination_bucket = 'robotics-sim-data-805791260265-us-west-2-csved'
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # Get date from source path
        parts = source_key.split('/')
        year_part = [part for part in parts if part.startswith('year=')][0]
        month_part = [part for part in parts if part.startswith('month=')][0]
        day_part = [part for part in parts if part.startswith('day=')][0]

        year = year_part.split('=')[1]
        month = month_part.split('=')[1]
        day = day_part.split('=')[1]

        for data_type, type_records in data_by_type.items():
            if not type_records:
                continue

            # Convert to CSV
            output = io.StringIO()
            writer = csv.DictWriter(output, fieldnames=type_records[0].keys())
            writer.writeheader()
            writer.writerows(type_records)

            # Define destination path
            dest_key = f"csv/year={year}/month={month}/day={day}/{data_type}/{timestamp}_{data_type}.csv"

            # Write to S3
            s3.put_object(
                Bucket=destination_bucket,
                Key=dest_key,
                Body=output.getvalue().encode('utf-8'),
                ContentType='text/csv'
            )

            print(f"‚úÖ Wrote {len(type_records)} {data_type} records to {dest_key}")

        return {
            'statusCode': 200,
            'body': f'Successfully processed {len(records)} records'
        }

    except Exception as e:
        print(f"‚ùå Error: {str(e)}")
        raise